{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/migdashn/Deep-Learning-Projects/blob/main/tts_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "UaLQnNzIlidG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "3DsS1a3WmlW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfL5UN8QiB7h"
      },
      "outputs": [],
      "source": [
        "from numpy.random import rand\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers.core.activation import Activation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator Class"
      ],
      "metadata": {
        "id": "xEdiymr5mfEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.module):\n",
        "    def __init__(self, seq_len=150, channels=3, num_classes=9, latent_dim=100, data_embed_dim=10, \n",
        "                label_embed_dim=10 ,depth=3, num_heads=5, \n",
        "                forward_drop_rate=0.5, attn_drop_rate=0.5):\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.channels = channels\n",
        "        self.num_classes = num_classes\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_embed_dim = data_embed_dim\n",
        "        self.label_embed_dim = label_embed_dim\n",
        "        self.depth = depth\n",
        "        self.num_heads = num_heads\n",
        "        self.attn_drop_rate = attn_drop_rate\n",
        "        self.forward_drop_rate = forward_drop_rate  \n",
        "        self.l1 = nn.Linear(self.latent_dim + self.label_embed_dim, self.seq_len * self.data_embed_dim)\n",
        "        self.label_embedding = nn.Embedding(self.num_classes, self.label_embed_dim) \n",
        "        self.blocks = Gen_TransformerEncoder(depth=self.depth,emb_size = self.data_embed_dim,\n",
        "                 num_heads = self.num_heads, drop_p = self.attn_drop_rate, forward_drop_p=self.forward_drop_rate)\n",
        "        self.deconv = nn.Sequential(nn.Conv2d(self.data_embed_dim, self.channels, 1, 1, 0))\n",
        "\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        c = self.label_embedding(labels)\n",
        "        x = torch.cat([z, c], 1)\n",
        "        x = self.l1(x)\n",
        "        x = x.view(-1, self.seq_len, self.data_embed_dim)\n",
        "        H, W = 1, self.seq_len\n",
        "        x = self.blocks(x)\n",
        "        x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
        "        output = self.deconv(x.permute(0, 3, 1, 2))\n",
        "        return output \n"
      ],
      "metadata": {
        "id": "ZWpO-cF7jGzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disciminator Class"
      ],
      "metadata": {
        "id": "V8iXucW5mvmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Sequential):\n",
        "    def __init__(self, \n",
        "                 in_channels=3,\n",
        "                 patch_size=15,\n",
        "                 data_emb_size=50,\n",
        "                 label_emb_size=10,\n",
        "                 seq_length = 150,\n",
        "                 depth=3, \n",
        "                 n_classes=9, \n",
        "                 **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding_Linear(in_channels, patch_size, data_emb_size, seq_length),\n",
        "            Dis_TransformerEncoder(depth, emb_size=data_emb_size, drop_p=0.5, forward_drop_p=0.5, **kwargs),\n",
        "            ClassificationHead(data_emb_size, 1, n_classes)\n",
        "        )"
      ],
      "metadata": {
        "id": "2GKTMC50mebF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Block"
      ],
      "metadata": {
        "id": "OMdpKevTnKMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gen_TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,emb_size,num_heads=5,drop_p=0.5,\n",
        "                 forward_expansion=4, forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p), nn.Dropout(drop_p))),\n",
        "            ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size),\n",
        "            FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "            nn.Dropout(drop_p))))\n",
        "\n",
        "\n",
        "class Gen_TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth=8, **kwargs):\n",
        "        super().__init__(*[Gen_TransformerEncoderBlock(**kwargs) for _ in range(depth)]) \n"
      ],
      "metadata": {
        "id": "B9Rb2IHXnJXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dis_TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size=100,\n",
        "                 num_heads=5,\n",
        "                 drop_p=0.,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.):\n",
        "      \n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p))),\n",
        "            ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p))))\n",
        "\n",
        "\n",
        "class Dis_TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth=8, **kwargs):\n",
        "        super().__init__(*[Dis_TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
      ],
      "metadata": {
        "id": "FHCCfUFpootk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size=100, adv_classes=2, cls_classes=10):\n",
        "        super().__init__()\n",
        "        self.adv_head = nn.Sequential(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, adv_classes)\n",
        "        )\n",
        "        self.cls_head = nn.Sequential(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, cls_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_adv = self.adv_head(x)\n",
        "        out_cls = self.cls_head(x)\n",
        "        return out_adv, out_cls\n"
      ],
      "metadata": {
        "id": "sqAXZo24m4Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "\n",
        "class PatchEmbedding_Linear(nn.Module):\n",
        "    def __init__(self, in_channels = 21, patch_size = 16, emb_size = 100, seq_length = 1024):\n",
        "        super().__init__()\n",
        "        #change the conv2d parameters here\n",
        "        self.projection = nn.Sequential(\n",
        "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)',s1 = 1, s2 = patch_size),\n",
        "            nn.Linear(patch_size*in_channels, emb_size)\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((seq_length // patch_size) + 1, emb_size))\n",
        "\n",
        "\n",
        "    def forward(self, x:Tensor) ->Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        #prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # position\n",
        "        x += self.positions\n",
        "        return x   "
      ],
      "metadata": {
        "id": "ilsQr8pRoMZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
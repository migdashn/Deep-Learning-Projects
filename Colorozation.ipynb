{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK5mc4pU6jFPvU01pvIES3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d428ff0c86014c1ca1a0dbea1240d933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d33fee337ddd45988469aecc2265cfef",
              "IPY_MODEL_09e8945381e14c3ca85559ac88a25628",
              "IPY_MODEL_d68f01fb6f374f548b89b2282a877aed"
            ],
            "layout": "IPY_MODEL_f33dc3f3cfad46ab89394288563c530c"
          }
        },
        "d33fee337ddd45988469aecc2265cfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30a3e59ad65e4e4b814ac20a19c9f471",
            "placeholder": "​",
            "style": "IPY_MODEL_d54e117d477a48f4b1c867abad6b45f7",
            "value": "100%"
          }
        },
        "09e8945381e14c3ca85559ac88a25628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc0cb9b416714f9f96e00910e4d19bcf",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_013a0af9925942a7bf21e7cadf8a97b9",
            "value": 169001437
          }
        },
        "d68f01fb6f374f548b89b2282a877aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc5efb25e98e47d9a3bba357cf9cc392",
            "placeholder": "​",
            "style": "IPY_MODEL_c270945926604d20ab2bfcea4d73e0b3",
            "value": " 169001437/169001437 [00:02&lt;00:00, 85498952.82it/s]"
          }
        },
        "f33dc3f3cfad46ab89394288563c530c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a3e59ad65e4e4b814ac20a19c9f471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54e117d477a48f4b1c867abad6b45f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc0cb9b416714f9f96e00910e4d19bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "013a0af9925942a7bf21e7cadf8a97b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc5efb25e98e47d9a3bba357cf9cc392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c270945926604d20ab2bfcea4d73e0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/migdashn/Deep-Learning-Projects/blob/main/Colorozation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "54CXuFElr7Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from sklearn.utils import shuffle\n",
        "from torchvision import datasets, transforms\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "21YRW0NTqwRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class."
      ],
      "metadata": {
        "id": "pxtnPthh2-8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-7swx0qvg2",
        "outputId": "6b0dc59d-8a65-48ef-ad60-1687885a4b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "d428ff0c86014c1ca1a0dbea1240d933",
            "d33fee337ddd45988469aecc2265cfef",
            "09e8945381e14c3ca85559ac88a25628",
            "d68f01fb6f374f548b89b2282a877aed",
            "f33dc3f3cfad46ab89394288563c530c",
            "30a3e59ad65e4e4b814ac20a19c9f471",
            "d54e117d477a48f4b1c867abad6b45f7",
            "dc0cb9b416714f9f96e00910e4d19bcf",
            "013a0af9925942a7bf21e7cadf8a97b9",
            "cc5efb25e98e47d9a3bba357cf9cc392",
            "c270945926604d20ab2bfcea4d73e0b3"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d428ff0c86014c1ca1a0dbea1240d933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cifar_data = datasets.CIFAR10('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO checks to the data to see how to use it"
      ],
      "metadata": {
        "id": "D9_xi-4RuBDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert a colorized Tensor to black and white, you can use the grayscale method, which takes a 3-channel image and converts it to a single-channel image by combining the values of the red, green, and blue channels using a weighted average. Here's an example of how you can convert a color image represented as a Tensor to a grayscale image using PyTorch:"
      ],
      "metadata": {
        "id": "gH0B4Bvh3zN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a color image Tensor\n",
        "color_tensor = torch.rand(3, 256, 256)\n",
        "\n",
        "# Convert the Tensor to grayscale using the grayscale method\n",
        "gray_tensor = torch.mean(color_tensor, dim=0, keepdim=True)"
      ],
      "metadata": {
        "id": "wuowF_AK3ym5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can use the following formula to convert RGB image to grayscale image where Y = 0.299 * R + 0.587 * G + 0.114 * B"
      ],
      "metadata": {
        "id": "uS6JKgq337J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray_tensor = 0.299 * color_tensor[0,:,:] + 0.587 * color_tensor[1,:,:] + 0.114 * color_tensor[2,:,:]"
      ],
      "metadata": {
        "id": "F_ziMFMR36ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO create a black and white data using one of the methods"
      ],
      "metadata": {
        "id": "D3AMQQTD3Zf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "nMxCOAx6sDZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, latent):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv = nn.Conv2d(input, input//2, kernel_size=(4,4), stride=(2,2))\n",
        "        self.norm = nn.BatchNorm2d(input//2)\n",
        "        self.lr = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self,inputs,ifNorm):\n",
        "        x = self.conv(inputs)\n",
        "        if ifNorm == True:\n",
        "          x = self.norm(x)\n",
        "        x = self.lr(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JNNdr9V6OPJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input, ifDrop):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv = nn.ConvTranspose2d(input, input*2, kernel_size=(4,4), stride=(2,2))\n",
        "        self.norm = nn.BatchNorm2d(input*2)\n",
        "        self.drop = nn.Dropout2d(0.5)\n",
        "        self.rel = nn.ReLU()\n",
        "\n",
        "    def forward(self,inputs,ifDrop):\n",
        "        x = self.conv(inputs)\n",
        "        x = self.norm(x)\n",
        "        if ifDrop == True:\n",
        "          x = self.drop(x)\n",
        "        x = self.rel(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "W--9wb3BOboJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encode = Encoder()\n",
        "    def forward(self, x):\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "We2MQ_7r5T9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/* u-net gan model using for pix2pix in pytorch */\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                        diffY // 2, diffY - diffY//2))\n",
        "        # for padding issues, see \n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self."
      ],
      "metadata": {
        "id": "Sdgq97lwF5aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator():\n",
        "    inputs = Input(shape=(256,256,3), name=\"InputLayer\")\n",
        "    down_stack = [downsample(64, apply_batchnorm=False),\n",
        "        downsample(128),\n",
        "        downsample(256),\n",
        "        downsample(512),\n",
        "        downsample(512),\n",
        "        downsample(512),\n",
        "        downsample(512),]\n",
        "    encoding = downsample(512)\n",
        "\n",
        "    up_stack = [upsample(512, apply_dropout=True),\n",
        "        upsample(512, apply_dropout=True),\n",
        "        upsample(512, apply_dropout=True),\n",
        "        upsample(512),\n",
        "        upsample(256),\n",
        "        upsample(128),\n",
        "        upsample(64),]\n",
        "\n",
        "    x = inputs \n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "    \n",
        "    x = encoding(x)\n",
        "\n",
        "    skips = reversed(skips)\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = concatenate([x, skip])\n",
        "    \n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    out = Conv2DTranspose(3, kernel_size=4, strides=2, kernel_initializer=init, activation='tanh', padding='same')\n",
        "\n",
        "    out = out(x)\n",
        "\n",
        "    gen = Model(inputs=inputs, outputs=out, name=\"Generator\")\n",
        "    return gen\n"
      ],
      "metadata": {
        "id": "hiBkYc2psCvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Block"
      ],
      "metadata": {
        "id": "8PPxxmRzsp-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample(filters, apply_batchnorm=True):\n",
        "\n",
        "    model = Sequential(\n",
        "        Conv2D(filters, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False),\n",
        "    )\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ri-4x8sfsooY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder BLock"
      ],
      "metadata": {
        "id": "rVNj9kTxstSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(filters, apply_dropout=False):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2DTranspose(\n",
        "        filters,\n",
        "        kernel_size=4,\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        kernel_initializer='he_normal',\n",
        "        use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    if apply_dropout:\n",
        "        model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(ReLU())\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Zbo3WC0lsvk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator Model"
      ],
      "metadata": {
        "id": "Ox0KzG2Vs10A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator():\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "\n",
        "    image = Input(shape=(256,256,3), name=\"ImageInput\")\n",
        "    target = Input(shape=(256,256,3), name=\"TargetInput\")\n",
        "    x = concatenate([image, target])\n",
        "\n",
        "    x = downsample(64, apply_batchnorm=False)(x)\n",
        "    x = downsample(128)(x)\n",
        "    x = downsample(512)(x)\n",
        "\n",
        "    x = ZeroPadding2D()(x)\n",
        "    x = Conv2D(512, kernel_size=4, strides=1, kernel_initializer=init, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = ZeroPadding2D()(x)\n",
        "\n",
        "    x = Conv2D(1, kernel_size=4, kernel_initializer=init)(x)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[image, target],\n",
        "        outputs=x,\n",
        "        name=\"Discriminator\"\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "w2BuQW0fs5oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
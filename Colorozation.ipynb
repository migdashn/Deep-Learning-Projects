{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+08BQkZLH7m1Rt7xNHyiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d428ff0c86014c1ca1a0dbea1240d933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d33fee337ddd45988469aecc2265cfef",
              "IPY_MODEL_09e8945381e14c3ca85559ac88a25628",
              "IPY_MODEL_d68f01fb6f374f548b89b2282a877aed"
            ],
            "layout": "IPY_MODEL_f33dc3f3cfad46ab89394288563c530c"
          }
        },
        "d33fee337ddd45988469aecc2265cfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30a3e59ad65e4e4b814ac20a19c9f471",
            "placeholder": "​",
            "style": "IPY_MODEL_d54e117d477a48f4b1c867abad6b45f7",
            "value": "100%"
          }
        },
        "09e8945381e14c3ca85559ac88a25628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc0cb9b416714f9f96e00910e4d19bcf",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_013a0af9925942a7bf21e7cadf8a97b9",
            "value": 169001437
          }
        },
        "d68f01fb6f374f548b89b2282a877aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc5efb25e98e47d9a3bba357cf9cc392",
            "placeholder": "​",
            "style": "IPY_MODEL_c270945926604d20ab2bfcea4d73e0b3",
            "value": " 169001437/169001437 [00:02&lt;00:00, 85498952.82it/s]"
          }
        },
        "f33dc3f3cfad46ab89394288563c530c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a3e59ad65e4e4b814ac20a19c9f471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54e117d477a48f4b1c867abad6b45f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc0cb9b416714f9f96e00910e4d19bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "013a0af9925942a7bf21e7cadf8a97b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc5efb25e98e47d9a3bba357cf9cc392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c270945926604d20ab2bfcea4d73e0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/migdashn/Deep-Learning-Projects/blob/main/Colorozation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "54CXuFElr7Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from sklearn.utils import shuffle\n",
        "from torchvision import datasets, transforms\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "21YRW0NTqwRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "link : [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ],
      "metadata": {
        "id": "pxtnPthh2-8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-7swx0qvg2",
        "outputId": "6b0dc59d-8a65-48ef-ad60-1687885a4b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "d428ff0c86014c1ca1a0dbea1240d933",
            "d33fee337ddd45988469aecc2265cfef",
            "09e8945381e14c3ca85559ac88a25628",
            "d68f01fb6f374f548b89b2282a877aed",
            "f33dc3f3cfad46ab89394288563c530c",
            "30a3e59ad65e4e4b814ac20a19c9f471",
            "d54e117d477a48f4b1c867abad6b45f7",
            "dc0cb9b416714f9f96e00910e4d19bcf",
            "013a0af9925942a7bf21e7cadf8a97b9",
            "cc5efb25e98e47d9a3bba357cf9cc392",
            "c270945926604d20ab2bfcea4d73e0b3"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d428ff0c86014c1ca1a0dbea1240d933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cifar_data = datasets.CIFAR10('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STL-10 dataset:\n",
        "\n",
        "*10 classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.\n",
        "\n",
        "*Images are 96x96 pixels, color.\n",
        "\n",
        "*500 training images (10 pre-defined folds), 800 test images per class.\n",
        "\n",
        "*100000 unlabeled images for unsupervised learning. These examples are   extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.\n",
        "\n",
        "STL-10 link : [STL-10](https://cs.stanford.edu/~acoates/stl10/)"
      ],
      "metadata": {
        "id": "34-oa8fNjNOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO checks to the data to see how to use it"
      ],
      "metadata": {
        "id": "D9_xi-4RuBDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert a colorized Tensor to black and white, you can use the grayscale method, which takes a 3-channel image and converts it to a single-channel image by combining the values of the red, green, and blue channels using a weighted average. Here's an example of how you can convert a color image represented as a Tensor to a grayscale image using PyTorch:"
      ],
      "metadata": {
        "id": "gH0B4Bvh3zN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a color image Tensor\n",
        "color_tensor = torch.rand(3, 256, 256)\n",
        "\n",
        "# Convert the Tensor to grayscale using the grayscale method\n",
        "gray_tensor = torch.mean(color_tensor, dim=0, keepdim=True)"
      ],
      "metadata": {
        "id": "wuowF_AK3ym5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can use the following formula to convert RGB image to grayscale image where Y = 0.299 * R + 0.587 * G + 0.114 * B"
      ],
      "metadata": {
        "id": "uS6JKgq337J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray_tensor = 0.299 * color_tensor[0,:,:] + 0.587 * color_tensor[1,:,:] + 0.114 * color_tensor[2,:,:]"
      ],
      "metadata": {
        "id": "F_ziMFMR36ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO create a black and white data using one of the methods"
      ],
      "metadata": {
        "id": "D3AMQQTD3Zf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "nMxCOAx6sDZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **use this to see the model : [kaggle source code](https://www.kaggle.com/code/utkarshsaxenadn/landscape-colorizer-pix2pixgan/notebook#Discriminator)**"
      ],
      "metadata": {
        "id": "7xyCGg8Lcial"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, latent):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv = nn.Conv2d(input, input//2, kernel_size=(4,4), stride=(2,2))\n",
        "        self.norm = nn.BatchNorm2d(input//2)\n",
        "        self.lr = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self,inputs,ifNorm):\n",
        "        x = self.conv(inputs)\n",
        "        if ifNorm == True:\n",
        "          x = self.norm(x)\n",
        "        x = self.lr(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JNNdr9V6OPJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input, ifDrop):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv = nn.ConvTranspose2d(input, input*2, kernel_size=(4,4), stride=(2,2))\n",
        "        self.norm = nn.BatchNorm2d(input*2)\n",
        "        self.drop = nn.Dropout2d(0.5)\n",
        "        self.rel = nn.ReLU()\n",
        "\n",
        "    def forward(self,inputs,ifDrop):\n",
        "        x = self.conv(inputs)\n",
        "        x = self.norm(x)\n",
        "        if ifDrop == True:\n",
        "          x = self.drop(x)\n",
        "        x = self.rel(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "W--9wb3BOboJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Generator, self).__init__()   \n",
        "        self.enc0 = nn.Sequential( nn.Conv2d(input, input//2, kernel_size=(4,4), stride=(2,2)),          #encoder without batchNorm\n",
        "                                          nn.LeakyReLU(0.2))\n",
        "        self.enc1 = nn.Sequential( nn.Conv2d(input, input//2, kernel_size=(4,4), stride=(2,2)),           #encoder with batchNorm\n",
        "                                          nn.BatchNorm2d(input//2),\n",
        "                                          nn.LeakyReLU(0.2))\n",
        "        self.dec0 = nn.Sequential(nn.ConvTranspose2d(input, input*2, kernel_size=(4,4), stride=(2,2)),    #decoder without dropout\n",
        "                                  nn.BatchNorm2d(input*2),\n",
        "                                  nn.ReLU())\n",
        "        self.dec1 = nn.Sequential(nn.ConvTranspose2d(input, input*2, kernel_size=(4,4), stride=(2,2)),    #decoder with dropout\n",
        "                                  nn.BatchNorm2d(input*2),\n",
        "                                  nn.Dropout2d(0.5),\n",
        "                                  nn.ReLU())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      x = inputs \n",
        "      \n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "We2MQ_7r5T9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator"
      ],
      "metadata": {
        "id": "Ox0KzG2Vs10A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.enc0 = nn.Sequential( nn.Conv2d(input, input//2, kernel_size=(4,4), stride=(2,2)),          #encoder without batchNorm\n",
        "                                          nn.LeakyReLU(0.2))\n",
        "        self.enc1 = nn.Sequential( nn.Conv2d(input, input//2, kernel_size=(4,4), stride=(2,2)),           #encoder with batchNorm\n",
        "                                          nn.BatchNorm2d(input//2),\n",
        "                                          nn.LeakyReLU(0.2))\n",
        "        self.zeropad = nn.ZeroPad2d()\n",
        "        self.conv1 = nn.Conv2d(input_size,kernel_size=(4,4), stride=(1,1))\n",
        "        self.conv2 = nn.Conv2d(input_size,kernel_size=(4,4))\n",
        "        self.batchnorm = nn.BatchNorm2d(input_size)\n",
        "        self.lr = nn.LeakyReLU(0.2)\n",
        "    def forward(self,inputs):\n",
        "        #cat?\n",
        "        x = self.enc0(inputs)\n",
        "        x = self.enc1(x)\n",
        "        x = self.enc1(x)\n",
        "        x = self.zeroPad(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.BatchNorm2d(x)\n",
        "        x = self.lr(x)\n",
        "        x = self.zeropad(x)\n",
        "        x = self.conv2(x)\n",
        "        return x;\n"
      ],
      "metadata": {
        "id": "B2EN9E_jbPG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "sVHmMFwNg5ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **taken fron nir's lecture need to modify**"
      ],
      "metadata": {
        "id": "KOLoAhxohGQk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDKjMkFuhUch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(G, D, train_loader, z_size=100, lr=0.001, num_epochs=5): \n",
        "    d_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
        "    g_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    # keep track of loss and generated, \"fake\" samples\n",
        "    samples = []\n",
        "    losses = []\n",
        "\n",
        "    print_every = 400\n",
        "\n",
        "    # Get some fixed data for sampling. These are images that are held\n",
        "    # constant throughout training, and allow us to inspect the model's performance\n",
        "    sample_size=16\n",
        "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "    fixed_z = torch.from_numpy(fixed_z).float()\n",
        "\n",
        "    # train the network\n",
        "    D.train()\n",
        "    G.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "                    \n",
        "            batch_size = real_images.size(0)\n",
        "            \n",
        "            ## Important rescaling step - since the generator output layer is tanh ## \n",
        "            real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n",
        "            \n",
        "            # ============================================\n",
        "            #            TRAIN THE DISCRIMINATOR\n",
        "            # ============================================\n",
        "            \n",
        "            d_optimizer.zero_grad()\n",
        "            \n",
        "            # 1. Train with real images\n",
        "\n",
        "            # Compute the discriminator losses on real images \n",
        "            # smooth the real labels\n",
        "            D_real = D(real_images)\n",
        "            d_real_loss = real_loss(D_real, smooth=True)\n",
        "            \n",
        "            # 2. Train with fake images\n",
        "            \n",
        "            # Generate fake images\n",
        "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "            z = torch.from_numpy(z).float()\n",
        "            fake_images = G(z)\n",
        "            \n",
        "            # Compute the discriminator losses on fake images        \n",
        "            D_fake = D(fake_images)\n",
        "            d_fake_loss = fake_loss(D_fake)\n",
        "            \n",
        "            # add up loss and perform backprop\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "            \n",
        "            \n",
        "            # =========================================\n",
        "            #            TRAIN THE GENERATOR\n",
        "            # =========================================\n",
        "            g_optimizer.zero_grad()\n",
        "            \n",
        "            # 1. Train with fake images and flipped labels\n",
        "            \n",
        "            # Generate fake images\n",
        "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "            z = torch.from_numpy(z).float()\n",
        "            fake_images = G(z)\n",
        "            \n",
        "            # Compute the discriminator losses on fake images using flipped labels\n",
        "            D_fake = D(fake_images)\n",
        "            g_loss = real_loss(D_fake) # use real loss to flip labels\n",
        "            \n",
        "            # perform backprop\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # Print some loss stats\n",
        "            if batch_i % print_every == 0:\n",
        "                # print discriminator and generator loss\n",
        "                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                        epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "        \n",
        "        ## AFTER EACH EPOCH##\n",
        "        # append discriminator loss and generator loss\n",
        "        losses.append((d_loss.item(), g_loss.item()))\n",
        "        \n",
        "        # generate and save sample, fake images\n",
        "        G.eval() # eval mode for generating samples\n",
        "        samples_z = G(fixed_z)\n",
        "        samples.append(samples_z)\n",
        "        G.train() # back to train mode\n",
        "\n",
        "    # plot learning curve\n",
        "    fig, ax = plt.subplots()\n",
        "    losses = np.array(losses)\n",
        "    plt.plot(losses.T[0], label='Discriminator')\n",
        "    plt.plot(losses.T[1], label='Generator')\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.legend()     \n",
        "     \n",
        "    return samples"
      ],
      "metadata": {
        "id": "sqpKe4jAg5Kb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}